[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Brownian Motion.html",
    "href": "Brownian Motion.html",
    "title": "Stochastic Calculus",
    "section": "",
    "text": "Brownian Motion\n\nlibrary(ggplot2)\nlibrary(gganimate)\n\nNo renderer backend detected. gganimate will default to writing frames to separate files\nConsider installing:\n- the `gifski` package for gif output\n- the `av` package for video output\nand restarting the R session\n\n\nTo simulate a Brownian motion sample path \\(W(t)\\) on the interval \\([0, T]\\) we can adopt the following steps:\n\nChoose an integer \\(n\\) and let \\(\\Delta = T/n\\), so that \\(t_{i} = i\\Delta t\\) for \\(i = 0, 1,\\dots,n\\).\nGenerate a sequence \\(\\varepsilon_{i},\\dots,\\varepsilon_{n}\\) of iid standard normal rvs:\n\n\nGenerate a sequence \\(U_{1}, \\dots, U_{n}\\) of Uniform r.v.’s in the interval \\((0,1)\\).\nSet \\(\\varepsilon_{i} = \\Phi^{-1}(x)\\) where \\(\\Phi^{-1}(x)\\) is the inverse cumulative distribution of the standard Gaussian distribution.\nSet \\(dWi = \\varepsilon_{i} \\sqrt{\\Delta t}\\).\n\n\nFinally recursively construct the sample path of the BM letting:\n\n\n\\(W (0) = 0\\);\n\\(W (t_{i}) = W (t_{i-1}) + dWi , \\quad i = 1,\\dots, n\\)\n\n\n# Simulate increments of the BM setting\n\nset.seed(1)\n\nsim = 100 # number of simulations\n\nn = 250 # number of steps\n\nT = 1 # Time-to-maturity\n\n# Simulate Brownian motion process:\n\nBM <- function(T , n, sim){\n  \n  dt = seq(from = 0, to = T, by = T/n) # time-step\n  \n  path = lapply(1:sim, function(j){\n    \n    Ui = runif(n = length(dt), min = 0, max = 1) # sequence of uniform rv's\n    \n    varepsilon = qnorm(p = Ui, mean = 0, sd = 1) # sequence of gaussian rv's\n    \n    dWi = varepsilon*sqrt(dt) #\n    \n    dW = cumsum(dWi)\n    \n    ts.plot(dW)\n    \n    return(dW)\n    \n    })\n  \n  return()\n}"
  },
  {
    "objectID": "Empirical Data.html",
    "href": "Empirical Data.html",
    "title": "Empirical Data",
    "section": "",
    "text": "Distribution of risk measures\n\nWe calculate the quantiles, expectiles and extremiles from the standard residuals:\n\nFirst, we compute the risk measures via QMLE at \\(\\tau =\\{1\\%,5\\%,10\\%\\}\\).\nSecond, we compute the risk measures via (QMLE) bootstrap at \\(\\tau =\\{1\\%,5\\%,10\\%\\}\\).\n\n\n \n\n\nEmpirical Data\nWe collect the daily price data of S\\&P 500, EUR/USD, BTC/USD, ETH-USD, BNB-USD, DOGE-USD and ADA-USD\nThe following dashboard summarizes all risk measures considering Historical, Parametric and Filtered Historical Simulation.\n \nFigure below show the Filtered Historical Simulation risk measures for several assets for \\(\\tau = 0.05\\).\n\nrisk.data = \n  readRDS('C:/Users/Olive/Documents/Github/Thesis/GARCH-Based Asymetric Least Squares for Financial Time Series/Dashboards/GARCH-Based  ALS Empirical Analysis/empirical/risk.data.RDS')\n\n\nrisk.data %&gt;%\n  filter(window %in% c('expanding')) %&gt;% \n  filter(type %in% c('Filtered Historical Simulation','Returns')) %&gt;%\n  filter(measure %in% c('VaR','Returns','Expectile','Extremile')) %&gt;%\n  filter(level %in% 0.05) %&gt;% \n  filter(!(ticker %in% c('EURUSD=X'))) %&gt;% \n  ggplot() +\n  geom_line(aes(x = date, y = estimate, color = measure, linetype = type), linewidth = 1) +\n  scale_color_manual(values = \n                       c('Returns' = 'black' ,\n                         'Extremile' = 'red',\n                         'Expectile' = 'blue',\n                         'VaR' = 'forestgreen',\n                         'Expected Shortfall' = 'purple')) +\n  scale_linetype_manual(\n    values = c('Returns' = 'solid',\n               'Filtered Historical Simulation' = 'longdash')) +\n  theme_bw() +\n  labs(x = 'Time', y = 'Returns, Risk Measures') +\n  facet_wrap(~ticker, scales = 'free') + \n  theme(legend.title = element_blank(),\n            legend.position = 'bottom',\n            legend.text = element_text(size = 18),\n            axis.title = element_text(size = 16),\n            strip.text = element_text(size = 16))\n\n\n\n\n\n\nBootstrap Confidence Interval\nThe following dashboard reports the bootstrap confidence interval of all Filtered Historical Simulation risk measures.\n \nFigure below shows the bootstrap confidence interval of the \\(\\tau\\)-th Extremile (\\(\\tau =0.05\\)) risk measure considering a bootstrap confidence level of \\(\\alpha = 0.05\\).\n\nci.data = \n  readRDS('C:/Users/Olive/Documents/Github/Thesis/GARCH-Based Asymetric Least Squares for Financial Time Series/Dashboards/GARCH-Based  ALS Empirical Analysis/empirical_ci/ci.data.RDS')\n\n\nci.data %&gt;%\n  filter(window %in% c('expand')) %&gt;% \n  filter(measure %in% c('Returns','Extremile')) %&gt;%\n  filter(level %in% 0.05) %&gt;% \n  filter(!(ticker %in% c('EURUSD=X'))) %&gt;% \n  fill(c(lower_bound,upper_bound)) %&gt;% \n  ggplot() +\n  geom_line(aes(x = date, y = estimate, color = measure), linetype = 'solid', linewidth = 1) +\n  geom_line(aes(x = date, y = upper_bound), linetype = 'dotted', color = 'darkred',linewidth = 1) +\n  geom_line(aes(x = date, y = lower_bound), linetype = 'dotted', color = 'darkred', linewidth = 1) +\n  scale_color_manual(\n    values = c('Returns' = 'black' ,\n               'Extremile' = 'darkgrey',\n               'Expectile' = 'blue',\n               'VaR' = 'forestgreen')) + \n  labs(x = 'Time', y = 'Returns, Risk Measures') +\n  theme_bw() +\n  facet_wrap(~ticker, scales = 'free') +\n  theme(legend.title = element_blank(),\n            legend.position = 'bottom',\n            legend.text = element_text(size = 18),\n            axis.title = element_text(size = 16),\n            strip.text = element_text(size = 16))\n\n\n\n\nLikewise, Figure below shows the bootstrap confidence interval of the \\(\\tau\\)-th Expectile (\\(\\tau =0.05\\)) risk measure considering a bootstrap confidence level of \\(\\alpha = 0.05\\).\n\nci.data %&gt;%\n  filter(window %in% c('expand')) %&gt;% \n  filter(measure %in% c('Returns','Expectile')) %&gt;%\n  filter(level %in% 0.05) %&gt;% \n  filter(!(ticker %in% c('EURUSD=X'))) %&gt;% \n  fill(c(lower_bound,upper_bound)) %&gt;% \n  ggplot() +\n  geom_line(aes(x = date, y = estimate, color = measure), linetype = 'solid', linewidth = 1) +\n  geom_line(aes(x = date, y = upper_bound), linetype = 'dotted', color = 'darkred',linewidth = 1) +\n  geom_line(aes(x = date, y = lower_bound), linetype = 'dotted', color = 'darkred', linewidth = 1) +\n  scale_color_manual(\n    values = c('Returns' = 'black' ,\n               'Extremile' = 'darkgrey',\n               'Expectile' = 'blue',\n               'VaR' = 'forestgreen')) + \n  labs(x = 'Time', y = 'Returns, Risk Measures') +\n  theme_bw() +\n  facet_wrap(~ticker, scales = 'free') +\n  theme(legend.title = element_blank(),\n            legend.position = 'bottom',\n            legend.text = element_text(size = 18),\n            axis.title = element_text(size = 16),\n            strip.text = element_text(size = 16))\n\n\n\n\n\n\nDrawdown\n\nequity_ticker &lt;- c('^GSPC','EURUSD=X','BTC-USD',\n                   'ETH-USD','BNB-USD','DOGE-USD','ADA-USD')\n\nstart_date &lt;- ymd(\"2017-01-01\")\n\nend_date &lt;- ymd(\"2023-06-01\")\n\ndata &lt;- \n  yf_get(equity_ticker,start_date,end_date) %&gt;%\n  select(ref_date,ticker,price_adjusted,ret_adjusted_prices) %&gt;%\n  rename_with(~ c('date','ticker','price','returns')) %&gt;%\n  drop_na() %&gt;%\n  group_by(ticker) %&gt;%\n  ungroup()\n\nCryptocurrencies reached their peak values during the market’s surge in late 2021. However, many digital assets experienced significant price corrections in the following year. In particular, Bitcoin reached its record-breaking all-time high of \\(\\$67,566.83\\) on November 8, 2021, although experienced a substantial decline in value, trading at \\(\\$15,787.28\\) on November 21, 2022.\n\ndata %&gt;% \n  ggplot() +\n  geom_line(aes(x = date, y = price)) +\n  labs(x = 'Date', y = 'Price') + \n  facet_grid(ticker ~ ., scales = 'free') +\n  theme_bw()\n\n\n\n\nThe figure below presents a comparative analysis of historical drawdowns among various cryptocurrencies, namely Bitcoin (BTC-USD), Bitcoin Cash (BTC/USD), Ethereum (ETH-USD), Binance Coin (BNB-USD), Dogecoin (DOGE-USD), and Cardano (ADA-USD), with respect to the S&P 500 and EUR/USD. It is evident that Cardano, Ethereum, Dogecoin, Bitcoin, and Binance Coin have experienced significant declines, surpassing 80% since reaching their respective peak values. In contrast, the drawdowns observed in the S&P 500 and EUR/USD have been relatively milder, hovering around 30%. These results show the substantial price fluctuations and volatility observed within the cryptocurrency market while highlighting the comparatively more stable performance of traditional financial benchmarks like the S&P 500 and EUR/USD.\n\ndata %&gt;% \n  pivot_wider(id_cols = date, names_from = ticker, values_from = returns)  %&gt;%\n  as.xts() %&gt;% \n  Drawdowns() %&gt;%\n  as.data.frame() %&gt;% \n  rownames_to_column(var = 'date') %&gt;% \n  mutate(date = as_date(date)) %&gt;% \n  pivot_longer(-date, names_to = 'ticker', values_to = 'drawdown') %&gt;% \n  ggplot() +\n  geom_line(aes(x = date, y = drawdown, color = ticker)) +\n  theme_bw() +\n  labs(x = 'Date', y = 'Drawdown') + \n  theme(legend.position = 'bottom',\n        legend.title = element_blank()) +\n   guides(col = guide_legend(nrow = 1))\n\n\n\n\n\n\nCodes for replication\n\nQMLE.bootstrap &lt;- function(fit, data, n.bootfit = 999, n.ahead = 1, tau = 0.05, alpha = 0.05){\n  \n  # initial paarameters\n  \n  n.bootpred = n.ahead \n  \n  N = length(data)\n  \n  set.seed(1)\n  \n  # ---------------------------------------\n  \n  # generate paths of equal length to data based on empirical re-sampling of z\n  # Pascual, Romo and Ruiz (2006) p.2296 equation (5)\n  \n  fz = as.numeric(residuals(fit))\n  \n  empz = matrix(0, ncol = n.bootfit, nrow = N)\n  \n  empz = apply(as.data.frame(1:n.bootfit), 1, FUN=function(i){\n    \n    sample(fz, N, replace = TRUE)\n    \n  })\n  \n  # presigma uses the same starting values as the original fit\n  # in paper they use alternatively the unconditional long run sigma \n  # Pascual, Romo and Ruiz (2006) p.2296 equation (5) (P.2296 paragraph 2  \"...marginal variance...\"\n  \n  coef = as.numeric(coef(fit))\n  \n  spec =  \n    ugarchspec(\n      mean.model = list(armaOrder = c(0, 0), include.mean = FALSE),\n      variance.model = list(model = 'sGARCH', garchOrder = c(1,1)),  \n      fixed.pars = list(\n        mu = 0, # our mu (intercept)\n        ar1 = 0, # our phi_1 (AR(1) parameter of mu_t)\n        ma1 = 0, # our theta_1 (MA(1) parameter of mu_t)\n        omega = coef[1], # our alpha_0 (intercept)\n        alpha1 = coef[2], # our alpha_1 (ARCH(1) parameter of sigma_t^2)\n        beta1 = coef[3])) # our beta_1 (GARCH(1) parameter of sigma_t^2)\n  \n  presigma = tail(sqrt(fitted(fit)),1)\n  \n  prereturns = tail(data, 1)\n  \n  preresiduals = tail(fz, 1) \n  \n  paths = ugarchpath(spec, \n                     n.sim = N, \n                     m.sim = n.bootfit, \n                     presigma = presigma,\n                     prereturns = prereturns,\n                     preresiduals = preresiduals,\n                     n.start = 0, \n                     custom.dist = list(name = \"sample\", distfit = as.matrix(empz)))\n  \n  fitlist = vector(mode=\"list\", length = n.bootfit)\n  \n  simseries = fitted(paths)\n  \n  nx = NCOL(simseries)\n  \n  # generate path based forecast values\n  # for each path we generate n.bootpred vectors of resampled data of length n.ahead\n  # Equation (6) in the PRR paper (again using z from original fit)\n  #-------------------------------------------------------------------------\n  \n  fitlist = lapply(as.list(1:nx), FUN = function(i){\n    \n    fit.boot = garchx(y = as.numeric(simseries[,i]), order = c(1,1))\n    \n    theta = coef(fit.boot)\n    \n    df = tibble('ID' = i,\n                'omega' = theta[1],\n                'alpha' = theta[2], \n                'beta' = theta[3],\n                'epsilon' = data[length(data):1],\n                'eta' = c(as.numeric(residuals(fit.boot)),NA), \n                'j' = seq(0,length(data)-1,1),\n                'sum' = beta^j*(lead(epsilon)^2 - omega/(1-alpha-beta))) %&gt;% \n      drop_na() %&gt;% \n      reframe(\n        ID = unique(ID),\n        omega = unique(omega),\n        alpha = unique(alpha),\n        beta = unique(beta),\n        epsilon = first(epsilon), \n        sum = sum(sum),\n        sigma2.hat = omega + alpha*epsilon^2 + beta*(omega/(1-alpha - beta) + alpha*sum),\n        extremile.hat = sqrt(sigma2.hat)*extremile(eta, probs = tau),\n        expectile.hat = sqrt(sigma2.hat)*expectile(eta, probs = tau),\n        VaR.hat = sqrt(sigma2.hat)*quantile(eta, probs = tau),\n        ES.hat = sqrt(sigma2.hat)*mean(if_else(eta &lt; quantile(eta, probs = tau),eta,NA),na.rm = TRUE))\n    \n    return(df)\n    \n  })\n  \n  confidence.interval = \n    fitlist %&gt;% \n    bind_rows() %&gt;% \n    select(ID,contains('hat')) %&gt;% \n    pivot_longer(-ID, names_to = 'measure', values_to = 'estimate') %&gt;% \n    group_by(measure) %&gt;% \n    summarise_at(vars(estimate), \n                 .funs = list(lower_bound = ~ quantile(., probs = alpha/2),\n                              upper_bound = ~ quantile(., probs = 1-alpha/2))) %&gt;% \n    left_join(\n      tibble(eta = as.numeric(residuals(fit)),\n             sigma2.hat = as.numeric(predict(fit,n.ahead = 1))) %&gt;%\n        reframe(\n          ID = 1,\n          sigma2.hat = unique(sigma2.hat),\n          extremile.hat = sqrt(sigma2.hat)*extremile(eta, probs = tau),\n          expectile.hat = sqrt(sigma2.hat)*expectile(eta, probs = tau),\n          VaR.hat = sqrt(sigma2.hat)*quantile(eta, probs = tau),\n          ES.hat = sqrt(sigma2.hat)*mean(if_else(eta &lt; quantile(eta, probs = tau),eta,NA),na.rm = TRUE)) %&gt;% \n        pivot_longer(-ID,names_to = 'measure', values_to = 'estimate') %&gt;% \n        select(-ID)) %&gt;% \n    relocate(measure,lower_bound,estimate,upper_bound)\n  \n  rm(fitlist)\n  \n  rm(paths)\n  \n  gc(verbose = FALSE)\n  \n  return(confidence.interval)\n  \n}"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Victor Henriques",
    "section": "",
    "text": "Welcome to my website"
  },
  {
    "objectID": "Simulation Results.html",
    "href": "Simulation Results.html",
    "title": "Simulation Results",
    "section": "",
    "text": "The main simulation results of GARCH-based risk measures as summarized as:\n\nIn general, we verify that a smaller sample size add more dispersion and quantile level skewness to the simulated distribution, regardless the scenario (Benchmark High Persistence), estimation method (Bootstrap, QMLE and MLE) and type of innovation (\\(t_{3},t_{8},t_{500}\\)).\nHeavy-tailed (\\(t_{3}\\)) simulation reveals that the estimation of risk measures via QMLE is biased.\n\nThe result is more pronounced in the High Persistence scenario.\nQMLE also divert from both Bootstrap and MLE estimation.\n\nThe distribution of Bootstrap, QMLE and MLE risk measures tends to approximate towards the simulated distribution when we choose thin-tailed distribution (\\(t_{500}\\)), regardless the simulation scenario.\nHeavy-tailed innovations (\\(t_{3}\\)) and high persistence (\\(\\beta = 0.89\\)) maximizes the difference in averages between QMLE and the simulation."
  },
  {
    "objectID": "Simulation Results.html#distribution-of-risk-measures",
    "href": "Simulation Results.html#distribution-of-risk-measures",
    "title": "Simulation Results",
    "section": "Distribution of risk measures",
    "text": "Distribution of risk measures\n\nWe calculate the quantiles, expectiles and extremiles from the standard residuals:\n\nFirst, we compute the risk measures via QMLE at \\(\\tau =\\{1\\%,5\\%,10\\%\\}\\).\nSecond, we compute the risk measures via MLE at \\(\\tau =\\{1\\%,5\\%,10\\%\\}\\).\nThird, we compute the risk measures via bootstrap at \\(\\tau =\\{1\\%,5\\%,10\\%\\}\\).\n\nIn the case of MLE, we estimate the GARCH model with t-Student innovations.\nIn the case of bootstrap, we follow Pascual, Romo, and Ruiz (2006)."
  },
  {
    "objectID": "Simulation Results.html#relative-distribution",
    "href": "Simulation Results.html#relative-distribution",
    "title": "Simulation Results",
    "section": "Relative Distribution",
    "text": "Relative Distribution\n\nFollowing the same strategy, we compute the relative distribution \\(\\xi_{\\tau}^{*} = \\widehat{\\xi}_{\\tau}/\\xi_{\\tau}\\)"
  },
  {
    "objectID": "Simulation Results.html#squared-relative-distribution",
    "href": "Simulation Results.html#squared-relative-distribution",
    "title": "Simulation Results",
    "section": "Squared Relative Distribution",
    "text": "Squared Relative Distribution\n\n\nFollowing the same strategy, we compute the squared relative distribution \\((\\xi_{\\tau}^{*})^{2} = (\\widehat{\\xi}_{\\tau}/\\xi_{\\tau})^{2}\\)"
  },
  {
    "objectID": "Simulation Results.html#simulation-1",
    "href": "Simulation Results.html#simulation-1",
    "title": "Simulation Results",
    "section": "Simulation",
    "text": "Simulation\n\nBenchmark with Gaussian Distribution\n\nConsider the following GARCH(1, 1) process for the returns:\n\n\\[\n\\begin{cases}\n\\epsilon_{t} = \\sigma_{t} \\eta_{t} \\ , \\quad \\eta_{t} \\thicksim t_{500}\\\\\n\\sigma_{t}^{2} = \\frac{20^2}{252} + 0.10\\epsilon_{t-1}^{2} + 0.80\\sigma_{t-1}^{2}\n\\end{cases}\n\\]\ngarch_benchmark_normal <-\n  ugarchspec(\n    mean.model = list(armaOrder = c(0,0), include.mean = FALSE), # ARMA order\n    variance.model = list(model = \"sGARCH\", garchOrder = c(1,1)), # GARCH order\n    distribution.model = \"std\", # Innovation distribution\n    fixed.pars = list(\n      mu = 0, # our mu (intercept)\n      ar1 = 0, # our phi_1 (AR(1) parameter of mu_t)\n      ma1 = 0, # our theta_1 (MA(1) parameter of mu_t)\n      omega = (20^2/252)*(1-0.1-0.8), # our alpha_0 (intercept)\n      alpha1 = 0.1, # our alpha_1 (ARCH(1) parameter of sigma_t^2)\n      beta1 = 0.8, # our beta_1 (GARCH(1) parameter of sigma_t^2)\n      shape = 500)) # d.o.f. nu for standardized t_nu innovations\n\n\nBenchmark with t-Student Distribution\n\nConsider the following GARCH(1, 1) process for the returns:\n\n\\[\n\\begin{cases}\n\\epsilon_{t} = \\sigma_{t} \\eta_{t} \\ , \\quad \\eta_{t} \\thicksim t_{8} \\\\\n\\sigma_{t}^{2} = \\frac{20^2}{252} + 0.10\\epsilon_{t-1}^{2} + 0.80\\sigma_{t-1}^{2}\n\\end{cases}\n\\]\ngarch_benchmark_t <-\n  ugarchspec(\n    mean.model = list(armaOrder = c(0,0), include.mean = FALSE), # ARMA order\n    variance.model = list(model = \"sGARCH\", garchOrder = c(1,1)), # GARCH order\n    distribution.model = \"std\", # Innovation distribution\n    fixed.pars = list(\n      mu = 0, # our mu (intercept)\n      ar1 = 0, # our phi_1 (AR(1) parameter of mu_t)\n      ma1 = 0, # our theta_1 (MA(1) parameter of mu_t)\n      omega = (20^2/252)*(1-0.1-0.8), # our alpha_0 (intercept)\n      alpha1 = 0.1, # our alpha_1 (ARCH(1) parameter of sigma_t^2)\n      beta1 = 0.8, # our beta_1 (GARCH(1) parameter of sigma_t^2)\n      shape = 8)) # d.o.f. nu for standardized t_nu innovations\n\n\nHigh persistence with Gaussian Distribution\n\nConsider the following GARCH(1, 1) process for the returns:\n\n\\[\n\\begin{cases}\n\\epsilon_{t} = \\sigma_{t} \\eta_{t} \\ , \\quad \\eta_{t} \\thicksim t_{500} \\\\\n\\sigma_{t}^{2} = \\frac{20^2}{252} + 0.10\\epsilon_{t-1}^{2} + 0.89\\sigma_{t-1}^{2}\n\\end{cases}\n\\]\ngarch_high_persistence_normal <-\n  ugarchspec(\n    mean.model = list(armaOrder = c(0,0), include.mean = FALSE), # ARMA order\n    variance.model = list(model = \"sGARCH\", garchOrder = c(1,1)), # GARCH order\n    distribution.model = \"std\", # Innovation distribution\n    fixed.pars = list(\n      mu = 0, # our mu (intercept)\n      ar1 = 0, # our phi_1 (AR(1) parameter of mu_t)\n      ma1 = 0, # our theta_1 (MA(1) parameter of mu_t)\n      omega = (20^2/252)*(1-0.1-0.89), # our alpha_0 (intercept)\n      alpha1 = 0.1, # our alpha_1 (ARCH(1) parameter of sigma_t^2)\n      beta1 = 0.89, # our beta_1 (GARCH(1) parameter of sigma_t^2)\n      shape = 500)) # d.o.f. nu for standardized t_nu innovations\n\n\nHigh persistence with t-Student distribution\n\nConsider the following GARCH(1, 1) process for the returns:\n\n\\[\n\\begin{cases}\n\\epsilon_{t} = \\sigma_{t} \\eta_{t} \\ , \\quad \\eta_{t} \\thicksim t_{8} \\\\\n\\sigma_{t}^{2} = \\frac{20^2}{252} + 0.10\\epsilon_{t-1}^{2} + 0.89\\sigma_{t-1}^{2}\n\\end{cases}\n\\]\ngarch_high_persistence_t <-\n  ugarchspec(\n    mean.model = list(armaOrder = c(0,0), include.mean = FALSE), # ARMA order\n    variance.model = list(model = \"sGARCH\", garchOrder = c(1,1)), # GARCH order\n    distribution.model = \"std\", # Innovation distribution\n    fixed.pars = list(\n      mu = 0, # our mu (intercept)\n      ar1 = 0, # our phi_1 (AR(1) parameter of mu_t)\n      ma1 = 0, # our theta_1 (MA(1) parameter of mu_t)\n      omega = (20^2/252)*(1-0.1-0.89), # our alpha_0 (intercept)\n      alpha1 = 0.1, # our alpha_1 (ARCH(1) parameter of sigma_t^2)\n      beta1 = 0.89, # our beta_1 (GARCH(1) parameter of sigma_t^2)\n      shape = 8)) # d.o.f. nu for standardized t_nu innovations"
  },
  {
    "objectID": "Simulation Results.html#computing-the-risk-measures",
    "href": "Simulation Results.html#computing-the-risk-measures",
    "title": "Simulation Results",
    "section": "Computing the risk measures",
    "text": "Computing the risk measures\nrisk.measures <- lapply(1:length(garch_simulation), function(i){\n  \n  garch.data = get(garch_simulation[i])\n  \n  garch.model = get(garch_model[i])\n\n  garch.risk <- lapply(1:10000,function(j){\n    \n    # Simulation\n    \n    returns = garch.data@path$seriesSim[,j]\n    \n    volatility = garch.data@path$sigmaSim[,j]\n    \n    residuals = garch.data@path$residSim[,j]\n    \n    std.residuals = (returns/volatility)\n    \n    # QMLE\n    \n    qmle.garch = garchx(y = returns, order = c(1,1))\n    \n    qmle.std.residuals = as.numeric(residuals.garchx(qmle.garch))\n    \n    # MLE\n    \n    mle.garch = ugarchfit(\n      spec = \n        ugarchspec(\n          mean.model = list(armaOrder = c(0,0), include.mean = FALSE), # ARMA order #\n          variance.model = \n            list(model = \"sGARCH\",garchOrder = c(1,1)), # GARCH order #\n          distribution.model = \"std\"), # Innovation distribution #\n      data = returns,\n      solver = 'hybrid')\n    \n    mle.std.residuals = as.numeric(residuals(mle.garch,standardize = TRUE))\n    \n    # Bootstrap \n    \n    garch.bootstrap =\n      bootstrap(fitORspec = mle.garch,\n                    data = returns,\n                    n.bootfit = 1,\n                    n.bootpred = 1,\n                    n.ahead = 1,\n                    rseed = c(1,2),\n                    solver = \"hybrid\",\n                    solver.control = list(), fit.control = list(),\n                    external.forecasts =  list(mregfor = NULL, vregfor = NULL),\n                    mexsimdata = NULL, vexsimdata = NULL)\n\n    boot.std.residuals = garch.bootstrap$boot.std.residuals\n\n    garch.results =\n      tibble(\n        id_simulation = j,\n        sim.std.residuals = std.residuals,\n        qmle.std.residuals = qmle.std.residuals,\n        mle.std.residuals = mle.std.residuals,\n        boot.std.residuals = boot.std.residuals) %>% \n      pivot_longer(-id_simulation, names_to = \"risk.measure\", values_to = \"residuals\") %>% \n      group_by(id_simulation,risk.measure) %>% \n      summarise_at(vars(residuals),\n                   .funs = list(\n                     quantile_0.010  = ~ quantile(., probs = 0.010),\n                     quantile_0.025  = ~ quantile(., probs = 0.025),\n                     quantile_0.050  = ~ quantile(., probs = 0.050),\n                     quantile_0.100  = ~ quantile(., probs = 0.100),\n                     quantile_0.250  = ~ quantile(., probs = 0.250),\n                     quantile_0.500  = ~ quantile(., probs = 0.500),\n                     expectile_0.010  = ~ expectile(., probs = 0.010),\n                     expectile_0.025  = ~ expectile(., probs = 0.025),\n                     expectile_0.050  = ~ expectile(., probs = 0.050),\n                     expectile_0.100  = ~ expectile(., probs = 0.100),\n                     expectile_0.250  = ~ expectile(., probs = 0.250),\n                     expectile_0.500  = ~ expectile(., probs = 0.500),\n                     extremile_0.010  = ~ extremile(., probs = 0.010),\n                     extremile_0.025  = ~ extremile(., probs = 0.025),\n                     extremile_0.050  = ~ extremile(., probs = 0.050),\n                     extremile_0.100  = ~ extremile(., probs = 0.100),\n                     extremile_0.250  = ~ extremile(., probs = 0.250),\n                     extremile_0.500  = ~ extremile(., probs = 0.500)\n                     )) %>% \n      ungroup()   \n\n    return(tryCatch(garch.results, error = function(e) NULL)) \n\n  })\n  \n  garch.risk = \n    garch.risk %>% \n    bind_rows() %>% \n    mutate(data = garch_simulation[i],\n           model = garch_model[i])\n\n  rm(garch.data,garch.model)\n\n  gc()\n  \n    return(tryCatch(garch.risk, error = function(e) NULL)) \n  \n  })"
  },
  {
    "objectID": "Simulation Results.html#bootstrap-procedure",
    "href": "Simulation Results.html#bootstrap-procedure",
    "title": "Simulation Results",
    "section": "Bootstrap procedure",
    "text": "Bootstrap procedure\n\nWe follow the bootstrap method of Pascual, Romo, and Ruiz (2006).\nEstimate GARCH by ML and compute the standardized residuals \\(\\hat{\\eta}_{t}^{*} = \\epsilon_{t}^{*}/\\sigma_{t}^{*}\\)\nGenerate bootstrap samples: \\(\\epsilon_{t}^{*}=\\eta_{t}^{*} \\widehat{\\sigma}_{t}^{*}\\), with \\(\\widehat{\\sigma}_{t}^{* 2}=\\widehat{\\omega}+\\widehat{\\alpha} L_{t-1}^{* 2}+\\widehat{\\beta} \\widehat{\\sigma}_{t-1}^{* 2}\\) where \\(\\eta_{t}^{*}\\) are random draws with replacement from \\(\\widehat{F}_{\\hat{\\eta}_{t}^{*}}\\) and \\(\\widehat{\\sigma}_{1}^{*2}= \\widehat{\\sigma}_{1}^{2}=\\widehat{\\omega} /(1-\\widehat{\\alpha}-\\widehat{\\beta})\\).\nCompute MLE for each bootstrap sample: \\(\\widehat{\\theta}^{*}=\\left(\\widehat{\\omega}^{*}, \\widehat{\\alpha}^{*}, \\widehat{\\beta}^{*}\\right)\\).\nCompute the standard residuals: \\(\\hat{\\eta}_{t}^{*} = \\epsilon_{t}^{*}/\\sigma_{t}^{*}\\)\n\nbootstrap = function(fitORspec, data = NULL, n.ahead = 10,\n                     n.bootfit = 100, n.bootpred = 500, rseed = NA, \n                     solver = \"solnp\", solver.control = list(), fit.control = list(), \n                     external.forecasts =  list(mregfor = NULL, vregfor = NULL), \n                     mexsimdata = NULL, vexsimdata = NULL){\n  \n  require(xts)\n\n  fit = fitORspec\n  \n  model = fit@model\n  \n  vmodel = fit@model$modeldesc$vmodel\n  \n  m = model$maxOrder\n  \n  data = model$modeldata$data\n  \n  N = model$modeldata$T\n  \n  ns = model$n.start\n  \n  if(is.na(rseed[1])){\n    sseed1 = NA\n    sseed2 = NA\n  } else{\n    if(length(rseed) < n.bootpred){\n      stop(\"\\nugarchboot-->error: seed length must equal n.bootpred + n.bootfit for full method\\n\")\n    } else {\n      sseed = rseed\n      sseed1 = sseed[1:n.bootfit]\n      sseed2 = sseed[(n.bootfit+1):(n.bootpred + n.bootfit)]\n    }\n  }\n  \n  # generate paths of equal length to data based on empirical re-sampling of z\n  # Pascual, Romo and Ruiz (2006) p.2296 equation (5)\n  \n  fz = fit@fit$z\n  \n  empz = matrix(sample(fz, N, replace = TRUE), ncol = n.bootfit, nrow = N)\n\n  # presigma uses the same starting values as the original fit\n  # in paper they use alternatively the unconditional long run sigma \n  # Pascual, Romo and Ruiz (2006) p.2296 equation (5) (P.2296 paragraph 2  \"...marginal variance...\"\n  \n  paths = ugarchsim(fit, n.sim = N, m.sim = n.bootfit, \n                    presigma = tail(fit@fit$sigma, m), \n                    prereturns = tail(model$modeldata$data[1:N], m), \n                    preresiduals = tail(residuals(fit), m), \n                    startMethod = \"sample\", \n                    custom.dist = list(name = \"sample\", distfit = as.matrix(empz)),\n                    rseed = sseed1, mexsimdata = mexsimdata, vexsimdata = vexsimdata)\n  \n  fitlist = vector(mode = \"list\", length = n.bootfit)\n  \n  simseries = fitted(paths)\n  \n  spec = getspec(fit)\n  \n  # help the optimization with good starting parameters\n  \n  spec@model$start.pars = as.list(coef(fit))\n  \n  nx = NCOL(simseries)\n  \n  # get the distribution of the parameters (by fitting to the new path data)\n  #-------------------------------------------------------------------------\n  \n  fitlist = \n    ugarchfit(\n      spec = spec,\n      data = xts(as.numeric(simseries), as.Date(1:NROW(data), origin=\"1970-01-01\")),\n      solver = solver,\n      fit.control = fit.control, \n      solver.control = solver.control)\n    \n  boot.std.residuals = as.data.frame(residuals(fitlist, standardize = TRUE))\n  \n  ans = list(boot.std.residuals = boot.std.residuals)\n  \n  return(ans)\n  \n}"
  },
  {
    "objectID": "Conditional Expected Shortfall.html",
    "href": "Conditional Expected Shortfall.html",
    "title": "Expected Shortfall",
    "section": "",
    "text": "Francq and Zakoı̈an (2015) derive a limit distribution for Value-at-Risk under a general setup for GARCH-type models, but they do not establish the asymptotic distribution for the Expected Shorfall. In contrast, Gao and Song (2008) provide an asymptotic theory for both VaR and ES, although they restrict their analysis to a standard GARCH (p,q) model.\nWe try to complement these two papers by extending the results of Gao and Song (2008) in the framework of Francq and Zakoı̈an (2015).\nTo be honest, there is nothing new here. However, we have the opportunity to complement with:\n\nExtend the Expected Shortfall for other GARCH-type specifications\n\n\n\nProvide confidence interval Let \\(\\widehat{\\Xi}_{\\tau}\\) denote a consistent estimator of the asymptotic variance \\(\\Xi_{\\tau}\\). Then, the delta method suggest a \\((1-\\gamma)\\%\\) confidence interval for \\(\\text{ES}_{t}(\\tau)\\): \\[\n\\hat{\\text{ES}}_{t}(\\tau) \\pm \\frac{\\Phi_{1-\\gamma / 2}^{-1}}{\\sqrt{n}}\\left\\{\\frac{\\partial \\tilde{\\sigma}_t\\left(\\hat{\\theta}_{n, \\tau}\\right)}{\\partial \\theta^{\\prime}} \\widehat{\\Xi}_\\tau \\frac{\\partial \\tilde{\\sigma}_t\\left(\\hat{\\theta}_{n, \\tau}\\right)}{\\partial \\theta}\\right\\}^{1/2}\n\\]\nExplore new ideas such as VHS"
  },
  {
    "objectID": "Conditional Expected Shortfall.html#monte-carlo-simulation",
    "href": "Conditional Expected Shortfall.html#monte-carlo-simulation",
    "title": "Expected Shortfall",
    "section": "Monte Carlo Simulation",
    "text": "Monte Carlo Simulation\n\n\n\n\n\ngarch_specification &lt;-\n  ugarchspec(\n    mean.model = list(armaOrder = c(0,0), include.mean = FALSE), # ARMA order #\n    variance.model = list(model = \"sGARCH\", garchOrder = c(1,1)), # GARCH order #\n    distribution.model = \"std\", # Innovation distribution #\n    fixed.pars = list(\n      mu = 0, # our mu (intercept)\n      ar1 = 0, # our phi_1 (AR(1) parameter of mu_t)\n      ma1 = 0, # our theta_1 (MA(1) parameter of mu_t)\n      omega = (20^2/252)*(1-0.1-0.8), # our alpha_0 (intercept)\n      alpha1 = 0.1, # our alpha_1 (ARCH(1) parameter of sigma_t^2)\n      beta1 = 0.8, # our beta_1 (GARCH(1) parameter of sigma_t^2)\n      shape = 500)) # d.o.f. nu for standardized t_nu innovations\n\ngarch_simulation &lt;-\n  ugarchpath(\n    spec = garch_specification,\n    n.sim = 1000, # sample size (length of simulated paths),\n    m.sim = 1000, # number of paths\n    n.start = 100, # The burn-in sample\n    rseed = 1) # the pseudo-random seed to generate the garch process\n\n\nepsilon = fitted(garch_simulation)\n\neta = epsilon/sigma(garch_simulation)\n\nmu.foo = function(x){mean(ifelse(x &lt; quantile(x, probs = 0.05), x,NA), na.rm = TRUE)}\n\nmu.par =\n  apply(eta,2,mu.foo) %&gt;% \n  as.data.frame() %&gt;% \n  rename_with(~'mu.true') %&gt;% \n  mutate(\n    id = row_number(),\n    alpha.true = 0.1,\n    beta.true = 0.8,\n    omega.true = (20^2/252)*(1-0.1-0.8),\n    K.alpha.true = 0.1*mu.true^2,\n    K.omega.true = (20^2/252)*(1-0.1-0.8)*mu.true^2)\n\ngarch.sim &lt;- lapply(1:1000,function(j){\n  \n  returns = as.numeric(epsilon[,j])\n  \n  garch.fit = garchx(y = returns, order = c(1,1))\n  \n  std.residuals = residuals.garchx(garch.fit)\n  \n  mu.tau = mean(ifelse(std.residuals &lt; quantile(std.residuals, probs = 0.05),\n                       std.residuals,NA), na.rm = TRUE)\n  \n  theta = coef.garchx(garch.fit)\n  \n  df = tibble(\n    id = j,\n    mu.hat = mu.tau,\n    omega.hat = theta[1],\n    alpha.hat = theta[2],\n    beta.hat  = theta[3],\n    K.omega.hat = theta[1]*mu.tau^2,\n    K.alpha.hat = theta[2]*mu.tau^2)\n  \n  return(df)\n\n})\n\nsim = garch.sim %&gt;% bind_rows()\n\n\nsim %&gt;% \n  left_join(mu.par) %&gt;% \n  select(-contains('true')) %&gt;%\n  pivot_longer(-c(id), names_to = 'parameter', values_to = 'estimate') %&gt;% \n  ggplot(aes(estimate)) +\n  geom_histogram(position = 'identity', color = 'white') +\n  facet_wrap(~factor(parameter,levels=c('omega.hat','alpha.hat','beta.hat','mu.hat',\n                                        'K.omega.hat','K.alpha.hat')), scales = 'free')\n\n\n\n\n\nsim %&gt;% \n  left_join(mu.par) %&gt;% \n  mutate(\n    mu = sqrt(T)*(mu.hat - mu.true),\n    omega = sqrt(T)*(omega.hat - omega.true),\n    alpha = sqrt(T)*(alpha.hat - alpha.true),\n    K.omega = sqrt(T)*(K.omega.hat - K.omega.true),\n    K.alpha = sqrt(T)*(K.alpha.hat - K.alpha.true),\n    beta  = sqrt(T)*(beta.hat - beta.true)) %&gt;%\n  select(id,omega,alpha,beta,K.omega,K.alpha,mu) %&gt;% \n  pivot_longer(-c(id), names_to = 'parameter', values_to = 'estimate') %&gt;% \n  ggplot(aes(estimate)) +\n  geom_histogram(position = 'identity', color = 'white') +\n  # stat_function(fun = dnorm, args = list(mean = 0, sd = ),aes(colour = 'Normal')) +\n  facet_wrap(~factor(parameter,levels=c('omega','alpha','beta','mu',\n                                        'K.omega','K.alpha')), scales = 'free')"
  },
  {
    "objectID": "Conditional ES.html",
    "href": "Conditional ES.html",
    "title": "Expected Shortfall",
    "section": "",
    "text": "Francq and Zakoı̈an (2015) derive a limit distribution for Value-at-Risk under a general setup for GARCH-type models, but they do not establish the asymptotic distribution for the Expected Shorfall. In contrast, Gao and Song (2008) provide an asymptotic theory for both VaR and ES, although they restrict their analysis to a standard GARCH (p,q) model.\nWe try to complement these two papers by extending the results of Gao and Song (2008) in the framework of Francq and Zakoı̈an (2015).\nTo be honest, there is nothing new here. However, we have the opportunity to complement with:\n\nExtend the Expected Shortfall for other GARCH-type specifications\n\n\n\nProvide confidence interval Let \\(\\widehat{\\Xi}_{\\tau}\\) denote a consistent estimator of the asymptotic variance \\(\\Xi_{\\tau}\\). Then, the delta method suggest a \\((1-\\gamma)\\%\\) confidence interval for \\(\\text{ES}_{t}(\\tau)\\): \\[\n\\hat{\\text{ES}}_{t}(\\tau) \\pm \\frac{\\Phi_{1-\\gamma / 2}^{-1}}{\\sqrt{n}}\\left\\{\\frac{\\partial \\tilde{\\sigma}_t\\left(\\hat{\\theta}_{n, \\tau}\\right)}{\\partial \\theta^{\\prime}} \\widehat{\\Xi}_\\tau \\frac{\\partial \\tilde{\\sigma}_t\\left(\\hat{\\theta}_{n, \\tau}\\right)}{\\partial \\theta}\\right\\}^{1/2}\n\\]\nExplore new ideas such as VHS"
  },
  {
    "objectID": "Conditional ES.html#monte-carlo-simulation",
    "href": "Conditional ES.html#monte-carlo-simulation",
    "title": "Expected Shortfall",
    "section": "Monte Carlo Simulation",
    "text": "Monte Carlo Simulation\n\n\n\n\n\ngarch_specification &lt;-\n  ugarchspec(\n    mean.model = list(armaOrder = c(0,0), include.mean = FALSE), # ARMA order #\n    variance.model = list(model = \"sGARCH\", garchOrder = c(1,1)), # GARCH order #\n    distribution.model = \"std\", # Innovation distribution #\n    fixed.pars = list(\n      mu = 0, # our mu (intercept)\n      ar1 = 0, # our phi_1 (AR(1) parameter of mu_t)\n      ma1 = 0, # our theta_1 (MA(1) parameter of mu_t)\n      omega = (20^2/252)*(1-0.1-0.8), # our alpha_0 (intercept)\n      alpha1 = 0.1, # our alpha_1 (ARCH(1) parameter of sigma_t^2)\n      beta1 = 0.8, # our beta_1 (GARCH(1) parameter of sigma_t^2)\n      shape = 500)) # d.o.f. nu for standardized t_nu innovations\n\ngarch_simulation &lt;-\n  ugarchpath(\n    spec = garch_specification,\n    n.sim = 1000, # sample size (length of simulated paths),\n    m.sim = 1000, # number of paths\n    n.start = 100, # The burn-in sample\n    rseed = 1) # the pseudo-random seed to generate the garch process\n\n\nepsilon = fitted(garch_simulation)\n\neta = epsilon/sigma(garch_simulation)\n\nmu.foo = function(x){mean(ifelse(x &lt; quantile(x, probs = 0.05), x,NA), na.rm = TRUE)}\n\nmu.par =\n  apply(eta,2,mu.foo) %&gt;% \n  as.data.frame() %&gt;% \n  rename_with(~'mu.true') %&gt;% \n  mutate(\n    id = row_number(),\n    alpha.true = 0.1,\n    beta.true = 0.8,\n    omega.true = (20^2/252)*(1-0.1-0.8),\n    K.alpha.true = 0.1*mu.true^2,\n    K.omega.true = (20^2/252)*(1-0.1-0.8)*mu.true^2)\n\ngarch.sim &lt;- lapply(1:1000,function(j){\n  \n  returns = as.numeric(epsilon[,j])\n  \n  garch.fit = garchx(y = returns, order = c(1,1))\n  \n  std.residuals = residuals.garchx(garch.fit)\n  \n  mu.tau = mean(ifelse(std.residuals &lt; quantile(std.residuals, probs = 0.05),\n                       std.residuals,NA), na.rm = TRUE)\n  \n  theta = coef.garchx(garch.fit)\n  \n  df = tibble(\n    id = j,\n    mu.hat = mu.tau,\n    omega.hat = theta[1],\n    alpha.hat = theta[2],\n    beta.hat  = theta[3],\n    K.omega.hat = theta[1]*mu.tau^2,\n    K.alpha.hat = theta[2]*mu.tau^2)\n  \n  return(df)\n\n})\n\nsim = garch.sim %&gt;% bind_rows()\n\n\nsim %&gt;% \n  left_join(mu.par) %&gt;% \n  select(-contains('true')) %&gt;%\n  pivot_longer(-c(id), names_to = 'parameter', values_to = 'estimate') %&gt;% \n  ggplot(aes(estimate)) +\n  geom_histogram(position = 'identity', color = 'white') +\n  facet_wrap(~factor(parameter,levels=c('omega.hat','alpha.hat','beta.hat','mu.hat',\n                                        'K.omega.hat','K.alpha.hat')), scales = 'free')\n\n\n\n\n\nsim %&gt;% \n  left_join(mu.par) %&gt;% \n  mutate(\n    mu = sqrt(T)*(mu.hat - mu.true),\n    omega = sqrt(T)*(omega.hat - omega.true),\n    alpha = sqrt(T)*(alpha.hat - alpha.true),\n    K.omega = sqrt(T)*(K.omega.hat - K.omega.true),\n    K.alpha = sqrt(T)*(K.alpha.hat - K.alpha.true),\n    beta  = sqrt(T)*(beta.hat - beta.true)) %&gt;%\n  select(id,omega,alpha,beta,K.omega,K.alpha,mu) %&gt;% \n  pivot_longer(-c(id), names_to = 'parameter', values_to = 'estimate') %&gt;% \n  ggplot(aes(estimate)) +\n  geom_histogram(position = 'identity', color = 'white') +\n  # stat_function(fun = dnorm, args = list(mean = 0, sd = ),aes(colour = 'Normal')) +\n  facet_wrap(~factor(parameter,levels=c('omega','alpha','beta','mu',\n                                        'K.omega','K.alpha')), scales = 'free')"
  }
]